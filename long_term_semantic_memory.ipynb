{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bffb72a",
   "metadata": {},
   "source": [
    "# Long-Term Semantic Memory with LangGraph\n",
    "\n",
    "This notebook demonstrates how to implement long-term semantic memory using embeddings and vector search. \n",
    "\n",
    "## Key Concepts:\n",
    "- **Semantic memory**: Stores important facts extracted from conversations\n",
    "- **Embeddings**: Convert text to dense vectors for semantic similarity\n",
    "- **Vector store**: Retrieves relevant facts based on semantic meaning\n",
    "- **Memory extraction**: Automatically identify and store key information\n",
    "- **Retrieval**: Find relevant stored facts for context in future conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9237f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langgraph\",\n",
    "    \"langchain\",\n",
    "    \"langchain-openai\",\n",
    "    \"langchain-chroma\",\n",
    "    \"python-dotenv\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "\n",
    "print(\"âœ“ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6d4475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OpenAI API key loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Verify API key\n",
    "api_key = \" \"\n",
    "if not api_key:\n",
    "    print(\"âš ï¸  Warning: OPENAI_API_KEY not found. Please set it in your .env file\")\n",
    "else:\n",
    "    print(\"âœ“ OpenAI API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9e20d",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Embeddings and Vector Store\n",
    "\n",
    "Setup the semantic memory system with embeddings and a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0c65ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Embeddings and vector store initialized\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize embeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=api_key)\n",
    "    \n",
    "    # Initialize vector store (using Chroma for simplicity, stores in memory)\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"semantic_memory\",\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ Embeddings and vector store initialized\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error initializing embeddings: {e}\")\n",
    "    print(\"Make sure OPENAI_API_KEY is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e33a43",
   "metadata": {},
   "source": [
    "## Step 2: Define the Agent State\n",
    "\n",
    "The state now includes:\n",
    "- `messages`: Current conversation\n",
    "- `semantic_memory`: Stored facts from conversations (as text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c303b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent state defined with short-term and long-term memory\n"
     ]
    }
   ],
   "source": [
    "def add_messages(left: list[BaseMessage], right: list[BaseMessage]) -> list[BaseMessage]:\n",
    "    \"\"\"Append messages to existing messages.\"\"\"\n",
    "    return left + right\n",
    "\n",
    "def add_memories(left: list[str], right: list[str]) -> list[str]:\n",
    "    \"\"\"Append memories to existing memories.\"\"\"\n",
    "    return left + right\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    semantic_memory: Annotated[list[str], add_memories]\n",
    "\n",
    "print(\"âœ“ Agent state defined with short-term and long-term memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b61a8c",
   "metadata": {},
   "source": [
    "## Step 3: Create Agent Nodes\n",
    "\n",
    "1. **extract_memory**: Extract important facts from user messages\n",
    "2. **retrieve_memory**: Find relevant stored facts\n",
    "3. **agent**: Process conversation with retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d3aa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent nodes created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, openai_api_key=api_key)\n",
    "    \n",
    "    def extract_memory(state: AgentState) -> dict:\n",
    "        \"\"\"\n",
    "        Extract important facts from the latest user message.\n",
    "        Store them in the vector store for future retrieval.\n",
    "        \"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # Get the last user message\n",
    "        last_user_msg = None\n",
    "        for msg in reversed(messages):\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                last_user_msg = msg.content\n",
    "                break\n",
    "        \n",
    "        if not last_user_msg:\n",
    "            return {}\n",
    "        \n",
    "        print(\"ğŸ§  Extracting facts from message...\")\n",
    "        \n",
    "        # Use LLM to extract key facts with explicit criteria\n",
    "        extraction_prompt = f\"\"\"Extract facts matching these criteria:\n",
    "- Named entities (people, companies, places)\n",
    "- Preferences (likes, dislikes)\n",
    "- Skills or expertise mentioned\n",
    "- Goals or projects\n",
    "- Important dates or numbers\n",
    "\n",
    "Message: {last_user_msg}\n",
    "\n",
    "Format each fact on a new line as a simple statement. Only extract if there are facts matching the criteria.\"\"\"\n",
    "        \n",
    "        facts_response = llm.invoke(extraction_prompt)\n",
    "        facts_text = facts_response.content\n",
    "        \n",
    "        # Store each fact in vector store\n",
    "        if facts_text.strip() and \"no facts\" not in facts_text.lower():\n",
    "            facts = [f.strip() for f in facts_text.split(\"\\n\") if f.strip()]\n",
    "            if facts:\n",
    "                print(\"Storing {len(facts)} fact(s):\")\n",
    "                for fact in facts:\n",
    "                    print(f\"   â€¢ {fact}\")\n",
    "                vector_store.add_texts(facts)\n",
    "                return {\"semantic_memory\": facts}\n",
    "        \n",
    "        print(\"âŠ˜ No facts extracted\")\n",
    "        return {}\n",
    "    \n",
    "    def retrieve_memory(state: AgentState) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieve relevant facts from semantic memory based on current context.\n",
    "        \"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # Get the last message\n",
    "        last_msg = messages[-1].content if messages else \"\"\n",
    "        \n",
    "        if not last_msg or len(vector_store.get()[\"ids\"]) == 0:\n",
    "            return {}\n",
    "        \n",
    "        print(\"ğŸ” Retrieving relevant facts from memory...\")\n",
    "        \n",
    "        # Search for relevant facts\n",
    "        results = vector_store.similarity_search(last_msg, k=3)\n",
    "        \n",
    "        if results:\n",
    "            retrieved_facts = \"\\n\".join([doc.page_content for doc in results])\n",
    "            print(f\"âœ“ Found {len(results)} relevant fact(s)\")\n",
    "            return {\"semantic_memory\": [f\"[Retrieved] {retrieved_facts}\"]}\n",
    "        \n",
    "        print(\"âŠ˜ No relevant facts found\")\n",
    "        return {}\n",
    "    \n",
    "    def agent(state: AgentState) -> dict:\n",
    "        \"\"\"\n",
    "        Agent that processes conversation with semantic memory context.\n",
    "        \"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        semantic_memory = state.get(\"semantic_memory\", [])\n",
    "        \n",
    "        # Build context from semantic memory\n",
    "        context = \"\"\n",
    "        if semantic_memory:\n",
    "            context = f\"\\nRelevant facts from memory:\\n\" + \"\\n\".join(semantic_memory)\n",
    "        \n",
    "        # Prepare system message with memory context\n",
    "        system_msg = f\"\"\"You are a helpful assistant with access to long-term semantic memory.\n",
    "        \n",
    "{context}\n",
    "\n",
    "Use the facts from memory to provide better context-aware responses.\"\"\"\n",
    "        \n",
    "        # Call LLM with full context\n",
    "        response = llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            *messages\n",
    "        ])\n",
    "        \n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    print(\"âœ“ Agent nodes created successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error creating agent nodes: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2bfbc7",
   "metadata": {},
   "source": [
    "## Step 4: Build the Graph\n",
    "\n",
    "The workflow:\n",
    "1. Extract memory from user message\n",
    "2. Retrieve relevant facts from semantic memory\n",
    "3. Agent responds with memory-augmented context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d6c9aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Graph compiled successfully\n"
     ]
    }
   ],
   "source": [
    "# Create the graph\n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"extract_memory\", extract_memory)\n",
    "graph_builder.add_node(\"retrieve_memory\", retrieve_memory)\n",
    "graph_builder.add_node(\"agent\", agent)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"extract_memory\")\n",
    "graph_builder.add_edge(\"extract_memory\", \"retrieve_memory\")\n",
    "graph_builder.add_edge(\"retrieve_memory\", \"agent\")\n",
    "graph_builder.add_edge(\"agent\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "print(\"âœ“ Graph compiled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e6922",
   "metadata": {},
   "source": [
    "## Step 5: Run a Multi-Turn Conversation Demo\n",
    "\n",
    "Demonstrate how semantic memory persists across different conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4f8b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LONG-TERM SEMANTIC MEMORY DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "Turn 1: User Input\n",
      ">> I work as a software engineer at TechCorp and I love building AI systems.\n",
      "\n",
      "ğŸ§  Extracting facts from message...\n",
      "ğŸ’¾ Storing 3 fact(s):\n",
      "   â€¢ - Named entity: TechCorp\n",
      "   â€¢ - Preference: loves building AI systems\n",
      "   â€¢ - Skill or expertise: software engineer\n",
      "ğŸ” Retrieving relevant facts from memory...\n",
      "âœ“ Found 3 relevant fact(s)\n",
      "Turn 1: Agent Response\n",
      "<< That's great to hear! Working as a software engineer at TechCorp must provide you with exciting opportunities, especially since you love building AI systems. Are there any specific projects or technologies you're currently working on in the AI field?\n",
      "\n",
      "ğŸ“š Semantic memory facts stored: 19\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 2: User Input\n",
      ">> What do you know about my job?\n",
      "\n",
      "ğŸ§  Extracting facts from message...\n",
      "ğŸ’¾ Storing 1 fact(s):\n",
      "   â€¢ No specific facts matching the criteria have been provided in the message.\n",
      "ğŸ” Retrieving relevant facts from memory...\n",
      "âœ“ Found 3 relevant fact(s)\n",
      "Turn 2: Agent Response\n",
      "<< You work as a software engineer at TechCorp, and you have a passion for building AI systems. If you're looking for more specific information or have any particular questions about your job or the field, feel free to ask!\n",
      "\n",
      "ğŸ“š Semantic memory facts stored: 20\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 3: User Input\n",
      ">> I'm starting a new project on recommendation systems.\n",
      "\n",
      "ğŸ§  Extracting facts from message...\n",
      "ğŸ’¾ Storing 1 fact(s):\n",
      "   â€¢ - The speaker is starting a new project on recommendation systems.\n",
      "ğŸ” Retrieving relevant facts from memory...\n",
      "âœ“ Found 3 relevant fact(s)\n",
      "Turn 3: Agent Response\n",
      "<< That sounds like an exciting project! Recommendation systems are a crucial part of many applications, helping to personalize user experiences. Are you focusing on any particular algorithms or technologies for this project? Also, do you have specific goals or challenges that you're aiming to tackle with the recommendation system?\n",
      "\n",
      "ğŸ“š Semantic memory facts stored: 21\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 4: User Input\n",
      ">> What are my interests and background?\n",
      "\n",
      "ğŸ§  Extracting facts from message...\n",
      "ğŸ’¾ Storing 1 fact(s):\n",
      "   â€¢ No specific facts match the provided criteria in the message \"What are my interests and background?\"\n",
      "ğŸ” Retrieving relevant facts from memory...\n",
      "âœ“ Found 3 relevant fact(s)\n",
      "Turn 4: Agent Response\n",
      "<< Your interests include building AI systems, which aligns with your work as a software engineer at TechCorp. You are currently starting a new project focused on recommendation systems, which indicates a strong interest in AI and machine learning applications. If there are more specific aspects of your background or interests you'd like to share or discuss, feel free to let me know!\n",
      "\n",
      "ğŸ“š Semantic memory facts stored: 22\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ“ Multi-turn conversation completed\n",
      "âœ“ Total facts stored in semantic memory: 22\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize conversation with semantic memory\n",
    "    initial_state = {\n",
    "        \"messages\": [],\n",
    "        \"semantic_memory\": []\n",
    "    }\n",
    "    \n",
    "    # Multi-turn conversation\n",
    "    conversation_turns = [\n",
    "        \"I work as a software engineer at TechCorp and I love building AI systems.\",\n",
    "        \"What do you know about my job?\",\n",
    "        \"I'm starting a new project on recommendation systems.\",\n",
    "        \"What are my interests and background?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"LONG-TERM SEMANTIC MEMORY DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    current_state = initial_state.copy()\n",
    "    \n",
    "    for i, user_input in enumerate(conversation_turns, 1):\n",
    "        print(f\"Turn {i}: User Input\")\n",
    "        print(f\">> {user_input}\")\n",
    "        print()\n",
    "        \n",
    "        # Add user message to state\n",
    "        current_state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "        \n",
    "        # Run the agent\n",
    "        result = graph.invoke(current_state)\n",
    "        current_state = result\n",
    "        \n",
    "        # Extract and display agent response\n",
    "        last_message = current_state[\"messages\"][-1]\n",
    "        print(f\"Turn {i}: Agent Response\")\n",
    "        print(f\"<< {last_message.content}\")\n",
    "        print()\n",
    "        \n",
    "        # Show semantic memory size\n",
    "        stored_count = len(vector_store.get()[\"ids\"])\n",
    "        print(f\"ğŸ“š Semantic memory facts stored: {stored_count}\")\n",
    "        print(\"-\" * 60)\n",
    "        print()\n",
    "    \n",
    "    print(\"âœ“ Multi-turn conversation completed\")\n",
    "    print(f\"âœ“ Total facts stored in semantic memory: {len(vector_store.get()['ids'])}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error running conversation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d24f1",
   "metadata": {},
   "source": [
    "## Key Takeaways: Long-Term Semantic Memory Pattern\n",
    "\n",
    "### How it Works:\n",
    "1. **Extract**: Important facts are automatically extracted from user messages using LLM\n",
    "2. **Embed**: Facts are converted to embeddings for semantic understanding\n",
    "3. **Store**: Embeddings are stored in a vector database (Chroma)\n",
    "4. **Retrieve**: When responding, relevant facts are retrieved using semantic similarity\n",
    "5. **Contextualize**: Retrieved facts augment the agent's context for better responses\n",
    "\n",
    "### Characteristics:\n",
    "- âœ… Remembers important facts across multiple conversations\n",
    "- âœ… Retrieves semantically similar facts (not just keyword matching)\n",
    "- âœ… Facts persist if vector store is persistent\n",
    "- âœ… Scales better than storing all messages\n",
    "- âœ… More cost-effective for long-term conversations\n",
    "- âŒ Requires extraction logic (LLM calls)\n",
    "- âŒ Information loss during fact extraction\n",
    "\n",
    "### Common Use Cases:\n",
    "- Personal assistant that remembers user preferences\n",
    "- Customer support with historical context\n",
    "- Research assistant that builds knowledge base\n",
    "- Agent that evolves understanding over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
