{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163f5c9a",
   "metadata": {},
   "source": [
    "# Short-Term Memory with LangGraph\n",
    "\n",
    "This notebook demonstrates how to implement short-term memory in agents using LangGraph. Short-term memory captures the conversation history within a single session, allowing the agent to reference recent messages and maintain context across multiple turns.\n",
    "\n",
    "## Key Concepts:\n",
    "- **Short-term memory**: Stores conversation history in the current session\n",
    "- **Message history**: Maintains all messages exchanged between user and agent\n",
    "- **Context window**: Limited to current conversation only\n",
    "- **Stateful agent**: Uses graph state to persist memory across steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f22deb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langgraph\",\n",
    "    \"langchain\",\n",
    "    \"langchain-openai\",\n",
    "    \"python-dotenv\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "print(\"âœ“ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88e1c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OpenAI API key loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Verify API key\n",
    "api_key = \" \"\n",
    "if not api_key:\n",
    "    print(\"âš ï¸  Warning: OPENAI_API_KEY not found. Please set it in your .env file\")\n",
    "else:\n",
    "    print(\"âœ“ OpenAI API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb1294",
   "metadata": {},
   "source": [
    "## Step 1: Define the Agent State\n",
    "\n",
    "The state contains:\n",
    "- `messages`: List of all messages in the conversation (both user and AI)\n",
    "- This represents our short-term memory buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7613ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent state defined with short-term memory (message history)\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Define reducer function for messages\n",
    "def add_messages(left: list[BaseMessage], right: list[BaseMessage]) -> list[BaseMessage]:\n",
    "    \"\"\"Append new messages to existing messages.\"\"\"\n",
    "    return left + right\n",
    "\n",
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "print(\"âœ“ Agent state defined with short-term memory (message history)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444db96",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the LLM and Create Node Functions\n",
    "\n",
    "The `agent` node will process the conversation history and generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ca9a583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent node created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, openai_api_key=api_key)\n",
    "    \n",
    "    # Define the agent node\n",
    "    def agent(state: AgentState) -> dict:\n",
    "        \"\"\"\n",
    "        Agent node that processes messages and generates responses.\n",
    "        Uses short-term memory (all messages in state) as context.\n",
    "        \"\"\"\n",
    "        # All messages are available in state.messages (short-term memory)\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # Call the LLM with the full message history\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        # Return the new message to be added to state\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    print(\"âœ“ Agent node created successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error initializing LLM: {e}\")\n",
    "    print(\"Make sure OPENAI_API_KEY is set in your .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb851291",
   "metadata": {},
   "source": [
    "## Step 3: Build the Graph\n",
    "\n",
    "Create a simple linear graph where:\n",
    "1. User adds a message\n",
    "2. Agent processes it\n",
    "3. Loop back to accept new messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b25193f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Graph compiled successfully\n"
     ]
    }
   ],
   "source": [
    "# Create the graph\n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"agent\", agent)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"agent\")\n",
    "graph_builder.add_edge(\"agent\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "print(\"âœ“ Graph compiled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba3f33a",
   "metadata": {},
   "source": [
    "## Step 4: Run a Multi-Turn Conversation Demo\n",
    "\n",
    "Demonstrate how the agent remembers context across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed2c6b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SHORT-TERM MEMORY DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "Turn 1: User Input\n",
      ">> My name is Alice and I'm learning about AI.\n",
      "\n",
      "ğŸ“Š Short-term memory size: 1 messages\n",
      "\n",
      "Turn 1: Agent Response\n",
      "<< Hi Alice! It's great to hear that you're learning about AI. There's a lot to explore in this field, from machine learning and natural language processing to robotics and neural networks. Do you have any specific topics or questions in mind that you'd like to discuss?\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 2: User Input\n",
      ">> What's my name?\n",
      "\n",
      "ğŸ“Š Short-term memory size: 3 messages\n",
      "\n",
      "Turn 2: Agent Response\n",
      "<< Your name is Alice! How can I assist you further, Alice?\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 3: User Input\n",
      ">> What was the first thing I told you?\n",
      "\n",
      "ğŸ“Š Short-term memory size: 5 messages\n",
      "\n",
      "Turn 3: Agent Response\n",
      "<< The first thing you told me was that your name is Alice and that you're learning about AI. If you have more questions or topics you'd like to explore, feel free to share!\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Turn 4: User Input\n",
      ">> Can you summarize our conversation so far?\n",
      "\n",
      "ğŸ“Š Short-term memory size: 7 messages\n",
      "\n",
      "Turn 4: Agent Response\n",
      "<< Certainly! So far, you introduced yourself as Alice and mentioned that you are learning about AI. I responded by expressing my enthusiasm and asking if you had specific topics or questions in mind. You then asked me about your name, and I confirmed it. Finally, you requested a summary of our conversation. If there's anything more you'd like to discuss or explore, just let me know!\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ“ Multi-turn conversation completed successfully\n",
      "âœ“ Total messages in short-term memory: 8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize conversation\n",
    "    initial_state = {\"messages\": []}\n",
    "    \n",
    "    # Multi-turn conversation\n",
    "    conversation_turns = [\n",
    "        \"My name is Alice and I'm learning about AI.\",\n",
    "        \"What's my name?\",\n",
    "        \"What was the first thing I told you?\",\n",
    "        \"Can you summarize our conversation so far?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"SHORT-TERM MEMORY DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    current_state = initial_state.copy()\n",
    "    \n",
    "    for i, user_input in enumerate(conversation_turns, 1):\n",
    "        print(f\"Turn {i}: User Input\")\n",
    "        print(f\">> {user_input}\")\n",
    "        print()\n",
    "        \n",
    "        # Add user message to state\n",
    "        current_state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "        \n",
    "        # Show current short-term memory size\n",
    "        print(f\"ğŸ“Š Short-term memory size: {len(current_state['messages'])} messages\")\n",
    "        print()\n",
    "        \n",
    "        # Run the agent\n",
    "        result = graph.invoke(current_state)\n",
    "        current_state = result\n",
    "        \n",
    "        # Extract and display agent response\n",
    "        last_message = current_state[\"messages\"][-1]\n",
    "        print(f\"Turn {i}: Agent Response\")\n",
    "        print(f\"<< {last_message.content}\")\n",
    "        print()\n",
    "        print(\"-\" * 60)\n",
    "        print()\n",
    "    \n",
    "    print(\"âœ“ Multi-turn conversation completed successfully\")\n",
    "    print(f\"âœ“ Total messages in short-term memory: {len(current_state['messages'])}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error running conversation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282ee60",
   "metadata": {},
   "source": [
    "## Key Takeaways: Short-Term Memory Pattern\n",
    "\n",
    "### How it Works:\n",
    "1. **State accumulates messages**: Each new message is appended to the messages list in state\n",
    "2. **Full context available**: The agent always has access to all messages in the current session\n",
    "3. **Reducer function**: The `add_messages` function ensures messages are properly accumulated\n",
    "4. **Stateless graph execution**: Each invoke processes from the current state\n",
    "\n",
    "### Characteristics:\n",
    "- âœ… Remembers all messages within a session\n",
    "- âœ… Allows agent to reference any previous message\n",
    "- âœ… Simple to implement\n",
    "- âœ… Cost increases with conversation length\n",
    "- âŒ No persistence across sessions\n",
    "- âŒ Memory grows indefinitely (may hit token limits)\n",
    "\n",
    "### Common Use Cases:\n",
    "- Chat applications within a single session\n",
    "- Question-answering over recent context\n",
    "- Clarification and follow-up handling\n",
    "- Interactive problem-solving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
